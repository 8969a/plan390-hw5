library(tidyverse)
library(tidytext)
library(textstem)
library(topicmodels)
library(reshape2)
speeches=read.csv("presidential_speeches_sample.csv")
tokenized = unnest_tokens(speeches, word, content)
tokenized = anti_join(tokenized, custom_stop_words, by="word")
tokenized = mutate(tokenized, lemma=lemmatize_words(word))
word_counts = group_by(tokenized, document, lemma) %>% summarize(count=n())
word_matrix = cast_dtm(wcounts, document, lemma, count)
model = LDA(word_matrix, 35, control=list(seed=18))